\chapter{Methods \label{Chapter_Methods}}
In order to design a prototype that uses multi-lingual documents as an input and classify the content of the document, we use the pre-trained model LiLT  \cite{wang-etal-2022-lilt}. According to author \cite{wang-etal-2022-lilt} \acrshort{lilt} is a \acrlong{lilt} and it can learn the language and layout knowledge from monolingual structured documents and generalize it to deal with multilingual documents, It can be pre-trained in one language and further fine-tune in any languages, this functionality makes it multi-lingual since regardless of the language \acrshort{lilt} has been pre-trained, it can be fine-tune using same methods in any language.

% During the pre-training of LiLT, the text and layout information are first decoupled and joint-optimized and re-coupled for fine-tunning. This techniques helps the model to learn text and layout jointly therefore named as \acrfull{lilt}, It can pre-trained on the structured documents of a single language and then directly be fine-tuned on other languages. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=1\textwidth]{chapters/images/Methods/Proposed Model/new_lilt_framework.png}
    \caption{LiLT framework \cite{wang-etal-2022-lilt}}
    \label{fig:lilt_framework}
\end{figure}

The overall representation of LiLT method is shown in \Cref{fig:lilt_framework}, Each component is explained in paragraphs below. 

\section{OCR Engines}
First step is two pass the image through OCR engines in order to extract the text bounding boxes and content from the document. For example OCR engines will generate list of words (['Bezeichnung,', 'Ort', 'und', 'Geschäftsnummer', 'des', 'Gerichts:', 'Erklärung‘,…]) and bounding boxes ([[64, 64, 147, 73], [152, 64, 170, 71], [175, 64, 197, 71], [203, 64, 315, 71], [319, 64, 340, 71], [345, 64, 401, 71], [135, 118, 239, 133],…]) from image where bounding boxes values represents as [x, y, width, height] from each word, the coordinates x and y refers to top-left corner of the bounding box. The overall functionality of some \acrshort{ocr} steps are described in upcoming paragraph in this section. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=1\textwidth, trim={0 20cm 0 0},clip]{chapters/images/de_train_0.jpg}
    \caption{Example document}
    \label{fig:exampleimage}
\end{figure}



\subsection{Line Finding}

The line finding algorithm was developed to be able to work with images that are skewed without performing de-skew in order to maintain  the image quality \cite{Line_Finding_Algorithem}. The key role of this algorithm is to perform blob filtering and line construction. It uses a simple percentile height filter for removing drop-caps and vertically touching characters, The median height approximation for text size and region and makes it safe to filter out blobs smaller than some fraction of the median height. By sorting x-coordinates, blobs are assigned to a unique text-lines. Later on, a least median of squares fir \cite{least_median_squares_algorithm} is being used for baseline estimation. Finally all blobs are being merged to form a line.

\subsection{Baseline Fitting}

After the line of the text document has been extracted, the baseline are fitted with more accuratly using a quadratic spline, an another first approch for OCR system that allowed Tesseract to handle curved baselines \cite{quadratic_spline_algorithm}. While quadratic spline have an advantage of claculating reasonably stable baseline, it can struggle when it comes to multiple spline segments. Therefore, the more traditional cubic spline \cite{Traditional_cubic_algorithm} has been used to perfom baseline fitting.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{chapters/images/OCR/Base_Line_Fitting.JPG}
    \caption{An example of a curved fitted baseline \cite{AnOverviewoftheTesseractOCREngine}}
    \label{fig:Baseline_Fitting}
\end{figure}

% \Cref{fig:Baseline_Fitting} shows the fitted baseline, descender line, mean-line and ascender line. The black line is straight and cyan line is slightly curved with compare to the straight black line above it.

\subsection{Fixed Pitch Detection and Chopping}

Tesseract takes the text lines and finds the fixed pitch and chops the words into characters using the pitch. It also disables the chopper and associator on these words for word recognition. An example of chopping fixed-pitch word has been showed in \Cref{fig:Fixed_Pitch_detection}.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.7\textwidth]{chapters/images/OCR/Fixed_Pitch_detection.JPG}
    \caption{Fixed Pitch Detection and Chopping\cite{AnOverviewoftheTesseractOCREngine}}
    \label{fig:Fixed_Pitch_detection}
\end{figure}

\subsection{Proportional Word Finding}

\Cref{fig:Proportional_Word_Finding} shows a typical exmple of the problems when it comes to perform various task mention in previous paragraphs. For instance, (\RomanNumeralLows{1}) The units of '11.9\%' is clearly larger than the kerned of words 'erated'. (\RomanNumeralLows{2}) There is no horizontal gap at all between words 'of' and 'financial'.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{chapters/images/OCR/Word_Finding.JPG}
    \caption{Proportional Word Finding\cite{AnOverviewoftheTesseractOCREngine}}
    \label{fig:Proportional_Word_Finding}
\end{figure}

Tesseract uses measurements of gaps in limited vertical range between the baseline and mean line. The spaces with smallest threshold are made fuzzy which later will be classified in word recognition.



\subsection{Overview of post processes after Line and Word Finding}

Once the lines and words from the documents have been found, the step "Word Recognition" \cite{AnOverviewoftheTesseractOCREngine} is performed to identify the word segmentation which will be letter on classified. Tesseract performs "Chopping Joined Characters" in order to improve the results by chopping the blob based on the confidence derived from classifier. After the elimination of non potential chops, if the word is still not good enough, an associator makes an A* (best first) search based on  segmentation graph of possible combinations. This step can help Tesseract to identify the broken characters with more accuracy. Later on a "Static Character Classifier" \cite{AnOverviewoftheTesseractOCREngine} generates the 3-dimensional, (x, y, position, angle) with 50-100 features and the prototype features are 4-dimensional (x, y, position, angle, length) in a character. Which than will be used to perform classification to assign classes. The results of the Tesseract OCR is then exported to text, word or HTML format.

%######################################################################################### Embedddings
\section{Tokenization}

As we know computers can deal with numerical representations like numbers, matrices or vectors, same applies to the \acrshort{nlp} models. The use of \acrshort{ocr} made it possible to get text from the documents but in order to understand those words and sentences, we need to convert first into some kind of numerical representation so that model can understand. In this section the important concept of tokenization is described. Generally in natural language, we deal with sequences of letters and/or symbols, it can be consider as a sequence of semantically related words \cite{faber2012constructing}. \cite{manning2008introduction} defines tokenization as "sequence of character in some particular document that are grouped together as useful semantic unit for processing". This process of tokenization means to make a small chunk from a text into smaller sequences of characters that is known as tokens. For instance, in the context of a sentence, using a white-spaces to separate words itself is a type of tokenization where each words can represents as a token. In \Cref{fig:Tokenization}, a well known technique for tokenization proposed by \cite{schuster2012japanese} and \cite{wu2016google}, \textbf{WordPiece(WP)} is described. In this process, first all the single words are being collected and represented as $uni-grams$ for instance single characters (\Cref{fig:Tokenization}, Splits). This process will form a vocabulary that contains all the single uni-grams occurred at least once in the corpus. Once the vocabulary has been made, a unique value will be assigned to each token which will represent that token. The process of tokenization sometimes also refer as embedding.  

\begin{figure}[!ht]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=1 \textwidth]{chapters/images/Tokenization/Tokenization.png}
    \caption{An example of WordPiece tokenization, The hashtags in front of the characters denotes that the word does not begin with these tokens}
    \label{fig:Tokenization}
\end{figure}



%#########################################################################################          Transformer

\section{Transformers}

Self-attention has been used in tasks such as reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations \cite{cheng2016longshorttermmemory, parikh2016decomposable, paulus2017deep, lin2017structured}. Simple-language question answering and language modeling tasks were being done by using End-to-end memory networks based on recurrent attention mechanism \cite{sukhbaatar2015end}. The so-called Transformer architecture was introduced in 2017 \cite{vaswani2017attention} and since then it has gained remarkable attention in the machine learning community. GPT, BERT, GPT-2, DistilBERT, BART and T5 are some well known Transformers models \cite{radford2018improving, devlin2018bert, GPT_2, DistilBERT, T5}. These models also known as language models, trained on large amount of raw text. The transformer architecture is novel and soon became a dominant architecture in natural language understanding and natural language generation, surpassing convolutional neural networks and recurrent neural networks in terms of performance. In addition, the architecture is able to scale with the size of the model, it is able to perform parallel training, and it features long-range sequence capture.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.5\textwidth]{chapters/images/Transformer/Architecture.JPG}
    \caption{Model Architecture\cite{vaswani2017attention}}
    \label{fig:Model_Architecture}
\end{figure}

\subsection{Overview of transformer model architecture}

The transformer architecture is made of encoder and decoder as shown left and right respectively in \Cref{fig:Model_Architecture}. A comprehensive introduction of components and its functionality is described in paragraphs below.

\subsubsection{Input Embedding}
It is the first component in both encoder and decoder. This layer takes input sequence and convert it into vectors that is also known as continuous representation. It maps each word  and provide numerical value to each word.

\subsubsection{Positional encoding}
In this step, the positional information is being injected to the vector representation derived from input embedding layer. The transformer described in \cite{vaswani2017attention} uses wave functions to give positional information to input embedding by creating vectors for odd and even positions using cosine and sine function respectively (\Cref{eq:Wave_functions}).

\begin{equation}
    \label{eq:Wave_functions}
    PE_{(pos, 2i)} = \sin{\left(\frac{pos}{10000^\frac{2i}{d_{model}}}\right)}
\end{equation}
\[PE_{(pos, 2i+1)} = \cos{\left(\frac{pos}{10000^\frac{2i}{d_{model}}}\right)} \]

\subsubsection{Encoder Layer}




The encoder layer is made of two sub-layer, multi-headed attention followed by fully connected feed forward network (\Cref{fig:Model_Architecture}). Multi-headed attention layer uses self-attention that allows the model to associate each word to other words in input embedding. To achieve self-attention, input embedding are passed through three linear layers to obtain query, key and value vectors represented as Q, K and V in \Cref{fig:Scaled_Dot-product_Attention}. As an example, if a word or sentence we used to search something in Google search are queries, the websites that Google search will provide are keys and the content of the websites are Values. Matrix multiplication step uses these queries and keys to obtain score matrix. The score matrix shows how much attention a word should keep into other words. These scores are then divided using square root of the dimension of queries and keys to scale it down to obtain stable gradients during values multiplication. After that the softmax is being taken to get the attention weights or attention filter. Softmax is a function that takes each vectors from scale matrix, normalizes it and gives the probabilistic distribution so that each component in matrix will be in interval of (0,1). By doing so, the higher score will be heightened and lower scores will be depressed providing model confident values to attend words accordingly. Later on, these attention weights are multiplied with value vectors (\Cref{eq:Attention}). The higher softmax scores will keep the word that is more important and lower softmax scores will soften the words that are irrelevant. After that query, key and value vectors are splited into N vectors and applied to self-attention individually therefore known as multi-headed attention (\Cref{fig:multi_head}) computation where each self-attention process is called head. Each head will produce output vector which will be concatenated into a single vector. This way each head will learn something different than other layers. 
\begin{equation}
    Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
    \label{eq:Attention}
\end{equation}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.25\textwidth]{chapters/images/Transformer/Inside_Attention.JPG}
    \caption{Scaled Dot-product Attention \cite{vaswani2017attention}}
    \label{fig:Scaled_Dot-product_Attention}
\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.35\textwidth]{chapters/images/Transformer/Multi_head.png}
    \caption{Multi-Head Attention \cite{vaswani2017attention}}
    \label{fig:multi_head}
\end{figure}



The block "Feed Forward" in \Cref{fig:Model_Architecture} refers to the term multi-layered networks of neurons and information in these layers flows into one direction hence is pronounce feedforward. The network is made of input layer, hidden layer and output layer. In \Cref{fig:neuralnet} a simple neural network with two input, two hidden and output neurons has been shown. The goal is to find a weights for hidden layer and output layer in a way that network takes the input and compute with these weights assigned to hidden and output neurons and try to predict the desired or nearest output. In initial phase, these weights assigned to hidden layer and output layer can be random or the attention derived from attention mechanism. In the process Forward Pass, the input values are passed to hidden layers. Total input value will be derived at each hidden node and then being squash using activation function like ReLU \cite{ReLU}, tanh \cite{tanh}. These activation functions help neural network to be non-linear allowing neural networks to expand complex representations and functions that is not possible with a linear regression model. Weights shows the strength of the connection between layers and biases make sure that if the input value is 0 than in activation function the output is not 0. The process is repeated for the output layer using the output of hidden layer as a inputs. Once we receive the outputs at output layers based on initial weights, the total error is being calculated based on targeted values. Once we have the total error of the network the process "Backwards pass" starts in which algorithm like "backpropagation" is used to calculate the gradient according to the total error using "Chain rule" for each node. Sometimes the learning rate is being used to multiply this gradient with the learning rate and then subtracted from the initial weights resulting in new weights. Then the process is repeating but this time with the new weights and is being repeated until the network provide outputs near to the targeted value. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{chapters/images/Transformer/neuralnet.png}
    \caption{Diagram of neural network with different layers}
    \label{fig:neuralnet}
\end{figure}


All these operation serves the prime purpose of encoding the input into a continuous representation with attentions so that decoder can focus on suitable word in the input while decoding. One can stack the encoder layers to encode the inputs in order to further encode the information. By doing so each layer can learn different attention representation that boosts the predictive power of the transformer network. 

\subsubsection{Decoder layer}
The decoder is responsible to generate the text sequences from the output vectors from encoder. It has similar sub layers as the encoder one multi-headed attention layers and feed-forward layer. The decoder is auto regressive that uses the previous outputs as inputs and uses encoder outputs that includes the attention information from input.

The input goes to embedding layers to obtain positional embedding. After that, these position aware vectors goes through multi-headed attention layer to generate the scores which later will be used as a decoder input. The next multi-headed attention layers works slightly different. Due to the fact that decoders are auto regressive and omit the sequence word-by-word, a condition is applied where it uses the positions to insure that prediction for position $i$ can only depend on known outputs at positions less than $i$. This process also known as mask where all the position greater than $i$ will be assigned as negative infinity so that when the softmax will be performed, there will be zero attention scores for future tokens. The output of the first multi-headed attention layer is mask output vector which will be used as a values for second multi-headed attention and the encoder output will be taken as keys and queries. The output of second multi-headed attention layer goes through to feed forward where the output will go through linear layer that act as a classifier, Here, the softmax layer will provide probability score to each class between 1 and 0. The one with highest probability score will be taken as predicted word. The decoder can also be stacked N times that takes inputs from the encoder and the layers before it, which helps model to focus and extract on different combinations of attention and attention heads, resulting in a boost of predictive power.
% \begin{equation}
%     Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
%     \label{eq:Attention}
% \end{equation}





% Most used structure for transforming sequence is encoder-decoder. Where encoder uses input sequence as $(x_1, x_2, ..., x_n)$ and convert it into continuous representations (numerical representation) z = $(z_1, z_2, ..., z_n)$. The decoder takes this given z and generates an output sequence $(y_1, y_2, ..., y_n)$ using one element at a time. Model is auto-regressive at each steps \cite{graves2013generating}, that uses the former result as a additional input in order to generate the next result. The Transformer uses this overall architectures in addition with self-attention with connected encoder and decoder (left and right respectively in Figure \ref{fig:Model_Architecture}).






% The encoder contains 6 identical layers where each layers contains two sub-layers. The first layer is a multi-head self-attention mechanism and second layer is feed-forward network. Each sub-layers of this layers is using residual connections \cite{he2016deep} (please refer to \ref{residual_connection}) which will be normalized using normalization layer \cite{ba2016layer} resulting LayerNorm(x+Sublayer(x)). Sublayer(x) is the function implemented by sub-layer itself. All these residual connections, sub-layers and embedding layers produces outputs of 512 dimension. Decoder also contains 6 layers with its sub-layers. Decoder takes the output of Encoder layer and performs self-attention, uses residual connections with normalization. In Decoder, the self-attention sub-layer is modified where it uses the positions to insure that prediction for position $i$ can only depend on known outputs at positions less than $i$.

% \subsection{residual connections \label{residual_connection}}
% \cite{Residual_connection}




% \subsection{Attention Mechanism\label{attention}}
% Transformer became prime component in natural language processing due to its ability to process long text and generate results and text while maintaining the context. Figure \ref{fig:Scaled_Dot-product_Attention} shows different components of "Multi-Head Attention" layer in Figure \ref{fig:Model_Architecture}. Where Q, K and V represents Query, Key and Value respectively. As an example, Query can be a word or sentence we used to search something in google search, Keys can be the websites that google search will provide which is similar to query and Value can be the content of the websites. The similarity between Query and Key are computed using \textbf{Cosine Similarity} ranging from +1 to -1. In Figure \ref{fig:Model_Architecture} in both encoder and decoder, Inputs are simply embedding of words, a fixed-length vectors. Machine translation algorithm like RNN, CNN and LSTM takes one input embedding at a time in a sequential manner, while Transformers take all embedding as a whole, Hence the \textbf{Positional Encoding} has been introduced in order to add information of word order to the vectors. In paper \cite{vaswani2017attention}, positional encoding has been achieved using wave frequencies \ref{eq:Wave_functions} to provide each word a unique position embedding.





% Once we have position aware matrices of Query, Key and Value, the steps mentioned in Figure \ref{fig:Scaled_Dot-product_Attention} will be performed in which in the step "MatMul" the dot product of matrices Q and K is being taken in order to find similarity also known as providing score. In step "Scale" we divide the score in order to scale, \(\sqrt{d_k}\) is used in paper \cite{vaswani2017attention} where \(d_k\) represents dimension of keys. In the next step the Softmax is being taken in order to squash the results between 0 and 1 at this stage the resultant matrix is so called \textbf{Attention Filter}. Finally, multiplying this Attention Filter to Value matrix gives us the \textbf{Attention} \ref{eq:Attention}.

% \begin{equation}
%     Attention(Q,K,V) = softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V
%     \label{eq:Attention}
% \end{equation}

% \subsection{Feed Forward\label{feed forward}}
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.7\textwidth]{chapters/images/Transformer/neuralnet.png}
%     \caption{Diagram of neural network with different layers}
%     \label{fig:neuralnet}
% \end{figure}

% The block "Feed Forward" in figure \ref{fig:Model_Architecture} refers to the term multi-layered networks of neurons and information in these layers flows into one direction hence is pronounce "feedforward". The network is made of Input Layer, Hidden Layer and Output Layer. In Figure \ref{fig:neuralnet} a simple neural network with two input, two hidden and output neurons has been shown. The goal is to find a weights for hidden layer and output layer in a way that Network takes the input and compute with these weights assigned to hidden and output neurons and try to predict the desired or nearest output. In initial phase, these weights assigned to hidden layer and output layer can be random or the attention derived from attention mechanism. In the process Forward Pass, the input values are passed to hidden layers. Total input value will be derived at each hidden node and then being squash using activation function like ReLU \cite{ReLU}, tanh \cite{tanh}, Softmax and so on. These activation functions help neural network to be non-linear. Weights shows the strength of the connection between layers and biases make sure that if the input value is 0 than in activation function the output is not 0. The process is repeated for the output layer using the output of hidden layer as a inputs. Once we receive the outputs at output layers based on initial weights, the Total Error is being calculated based on targeted values. Once we have the total error of the network the process "Backwards pass" starts in which algorithm like "backpropagation" is used to calculate the gradient according to the total error using "Chain rule" for each node. Sometimes the learning rate is being used to multiply this gradient with the learning rate and then subtracted from the initial weights resulting in new weights. Then the process is repeating but this time with the new weights and is being repeated until the network provide outputs near to targeted value. 















 

\section{The Text and Layout Flow}
Once we have words and respective bounding boxes, these information is passed to corresponding Transformer-based architecture in order to obtain enhanced feature. The flow containing the content and layout information are referred as Text-flow and Layout-flow. Both text and layout information run in parallel transformer architecture separately, therefore author also refer this settings as parallel dual-stream Transformer.

In the text flow, all the text derived from the OCR, first tokenized (vectorized) and concatenated as a sequence from the top-left to bottom-right, for instance if a document is having sentences of n number, than text embedding will be \(S_t= (s_1, s_2, s_3,...s_n)\). In order to preserve the information such as start of the sentence and end of the sentence, special tokens $[CLS]$ and $[SEP]$ is added in the concatenated series. Once we have all tokenized and concatenated sequences from entire document, $S_t$ will be truncated or padded with extra token $[PAD]$, which is a process where we make all sentence embedding $E_{token}$  into same length. Finally 1D positional embedding $P_{1D}$ (as shown in \Cref{fig:lilt_framework} using \Cref{eq:Wave_functions}) will be added to each sequence $E_{token}$ using the layer normalization $LN$ as shown in \Cref{eq:text_embedding} resulting in text embedding.
\begin{equation}
    E_T = LN(E_{token} + P_{1D})
    \label{eq:text_embedding}
\end{equation}

For layout flow, the process of preparing layout embedding is similar to the process we discussed in previous paragraph for text embedding. First a 2D positional sequence $S_l$ is being prepared from bounding boxes derived from OCR engines, the coordinates of all integers are first normalized in the range of [0, 1000]. Four embedding layers generates features using x-axis, y-axis, height and width resulting in bounding box \(B = (x_{min}, x_{max}, y_{min}, y_{max}, width, height)\). The special tokens $[CLS] = (0,0,0,0,0,0), [SEP] = (1000, 1000, 1000, 1000, 0, 0)$ and $[PAD] = (0,0,0,0,0,0)$ are attached in order to add information like start/end of the sentence and padding them to same length. After that channelwise concatenation operation follow up with  $Linear$ layer (\Cref{eq:Linear_Layer}) also known as dense layer are being performed in order to make 2D vectors into 1D. Later on these embedding will be added to 1D positional embedding as shown in \Cref{eq:layout_embedding}.

\begin{equation}
    \label{eq:Linear_Layer}
    P_{2D} = Linear(CAT(E_{x_{min}}, E_{x_{min}}, E_{x_{max}}, E_{y_{min}}, E_{y_{max}}, E_{width}, E_{height}))
\end{equation}

\begin{equation}
    \label{eq:layout_embedding}
    E_L = LN(P_{2D} + P_{1D})
\end{equation}




\subsection{BiACM}

Once the text embedding $E_T$ and layout embedding $E_L$ are calculated, both are feeded into sub-models respectively in order to obtain high-level enhanced features as shown in \Cref{fig:lilt_framework}. If there is no interaction between text and layout flow, text based transformer will learn only from text embedding and layout based transformer layer will only learn from layout embedding, this can impact on cross-modality if text and layout features derived from these transformer layers are just simply combined. To overcome this issue, \cite{wang-etal-2022-lilt} proposed new bi-directional attention complementation mechanismm (BiACM). This mechansim allows both transformer to exchange some information at early stages to improve cross-modality interaction across the complete encoding. The model uses vanila self-attention mechanism in transformer layers in order to capture the correlation between query $x_i$ and key $x_j$ using \Cref{eq:vanila_mechanism}.  
\begin{equation}
    \alpha_{ij} =  \frac{(x_iW^Q)(x_jW^K)^T}{\sqrt{d^h}}
    \label{eq:vanila_mechanism}
\end{equation}

For given $\alpha_{ij}^T$ and $\alpha_{ij}^L$ represents the text and layout attention scores for the same head of the same layer respectively, that will be shared as a common knowledge  as shown in \Cref{eq:BiACM}. $\alpha_{ij}^T$ is detached for $\widetilde{\alpha_{ij}^L}$ during pre-training where detach is a function that preserve the consistency of textual gradients by taking them out in the process of gradient computation. This helps reducing the effect on layout stream from textual stream during pre-training task. The final modified attention scores will be then used in both flows for further process as shown in \Cref{fig:lilt_framework}. 
\begin{equation}
    \widetilde{\alpha_{ij}^T} = \alpha_{ij}^L + \alpha_{ij}^T,
    \label{eq:BiACM}
\end{equation}
\[ \widetilde{\alpha_{ij}^L} = \left\{\begin{matrix}
 \alpha_{ij}^L + DETACH(\alpha_{ij}^T)&if\text{ Pre-train,}\\ 
 \alpha_{ij}^L + \alpha_{ij}^T&  if\text{ Fine-tune}
\end{matrix}\right.\]




\subsection{Pre-Training And Fine-Tuning}

The concept of pre-training comes from transfer learning \cite{Transfer_learning} and the core idea is to reuse previously learned knowledge from one or more task and apply it to a new task. The majority of deep learning methods are dependent on high quality labeled data. Labeling data manually can be expensive and time consuming. Pre-training methods allows models to use unlabeled data such as books, articles, websites and so on and helps models to identify patterns, structures and semantic knowledge that is present in the corpus. pre-training generally refers to train the models on these large amount of unlabeled corpus to enhance the initial parameters of the neural networks and fine-tune refers to further training these models on specific targeted task using supervised learning to improve performance in that sepcific tasks. The prime advantage of these methods was the ability to deal with more than one task for instance question answering, text classification, language generation and so on. In generative pre-training (GPT) \cite{radford2018improving} such approach has been discussed for language understanding task using transformer architecture and combination of unsupervised pre-training and supervised fine-tuning.

Pre-training has enabled a breakthrough in the domain of NLP. Prior to pre-trained model(PTM) there was a requirement of designing a models according to target specific tasks and these trained model can not be used other than that particular tasks. The emergenece of PTMs made it possible to serve models as a foundational model which started a new paradigm for NLP. For instance BERT, a pre-trained model from Google took 16 TPU (Tensor Processing Units) chips for \(BERT_{BASE}\) and 64 TPU chips for \(BERT_{LARGE}\) and 4 days for each model to complete the training \cite{devlin2018bert}. BERT is open-source and one can simply use it for downstream task with fine-tune in desired tasks and save amount of computation power and time it took to train this model.

\acrshort{lilt} \cite{wang-etal-2022-lilt} has been trained using tasks such as (\RomanNumeralLows{1}) Masked Visual-Language Modeling (\RomanNumeralLows{2}) Key Point Location (\RomanNumeralLows{3}) Cross-modal Alignment Identification. These tasks also refer as self-supervised pre-training tasks where unlabeled source data is used to train the network and later on this pre-trained network will be transferred to supervised learning (fine-tuning) using target dataset. These self-supervised pre-training tasks helps model to learn joint representations along with cross-modal cooperation autonomously in unlabeled dataset. A comprehensive description of these tasks are described in paragraphs below. 

\subsubsection{Masked Visual-Language Modeling}

This task was originally mentioned in \cite{devlin2018bert} as \acrfull{mlm}. \acrshort{mvlm} is simillar to \acrshort{mlm} but instead of dealing with only text input, it can work with text and layout information.  Masking is a process to inform the sequence-processing layers that some timestamps are missing in input and model is asked to predict over the whole vocabulary. The representation of this masked word or image region is being learned by model from left or right side of the words or regions. This approach are applied usually on the models that are bidirectional and over the training, model gets better to predict the masked word or region. In \acrshort{mvlm} during the pre-training of \acrshort{lilt} the masking is only applied on textual information while non-textual information remains unchanged. 15\% of the text tokens were masked, among them 80\% were replaces with special token $[MASK]$ and 10\% are replaced with random tokens from whole vocabulary and 10\% remained kept unchanged. 

\subsubsection{Key Point Location}
This task was introduced by \cite{wang-etal-2022-lilt} in order to make model understand layout information. \acrshort{kpl} divides the entire document layout into \(7\times7\) regions which will be randomly masked. The model have to predict the region for each boxes from classes like top-left corner, bottom-right corner and center point. This regions known as key points that helps model to fully understand the text content and know where to put a specific word/sentence based on surrounding word/sentence. During the pre-training in task \acrshort{kpl}, 15\% of boxes were masked, where 80\% of them were replaced by \((0,0,0,0,0,0)\), 10\% were replaced with random boxes sample from the same batch and rest 10\% were remained unchanged.

\subsubsection{Cross-modal Alignment Identification}
In this task both encoded features of token-box by \acrshort{mvlm} and \acrshort{kpl} are being collected. An additional binary classification head upon them is applied to identify whether the pair is aligned. 












%In addition, models like \cite{T5} is able to perform two tasks such as language understanding and generation task. 