\section{Pre-Training And Fine-Tunning}

The concept of pre-training comes from transfer learning \cite{Transfer_learning} and the core idea is to reuse previously learned knowledge from one or more task and apply it to a new task. The majority of deep learning methods are dependent on high quality labeled data. Labeling data manually can be expensive and time consuming. Pre-training methods allows models to use unlabeled data such as books, articles, websites and so on and helps models to identify patterns, structures and semantic knowledge that is present in the corpus. pre-training generally refers to train the models on these large amount of unlabeled corpus to enhance the initial parameters of the neural networks and fine-tune refers to further training these models on specific targeted task using supervised learning to improve performance in that sepcific tasks. The prime advantage of these methods was the ability to deal with more than one task for instance question answering, text classification, language generation and so on. In generative pre-training (GPT) \cite{radford2018improving} such approach has been discussed for language understanding task using transformer architecture and combination of unsupervised pre-training and supervised fine-tuning.

Pre-training has enabled a breakthrough in the domain of NLP. Prior to pre-trained model(PTM) there was a requirement of designing a models according to target specific tasks and these trained model can not be used other than that particular tasks. The emergenece of PTMs made it possible to serve models as a foundational model which started a new paradigm for NLP. For instance BERT, a pre-trained model from Google took 16 TPU (Tensor Processing Units) chips for \(BERT_{BASE}\) and 64 TPU chips for \(BERT_{LARGE}\) and 4 days for each model to complete the training \cite{devlin2018bert}. BERT is open-source and one can simply use it for downstream task with fine-tune in desired tasks and save amount of computation power and time it took to train this model.
%In addition, models like \cite{T5} is able to perform two tasks such as language understanding and generation task. 