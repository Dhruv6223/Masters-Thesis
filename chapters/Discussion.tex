\acrshort{lilt} is a \acrlong{lilt} and it can learn the layout knowledge from monolingual structured documents and generalize it to deal with multilingual documents \cite{wang-etal-2022-lilt}. It is called \acrlong{lilt} because it can be pre-trained in one language and further fine-tune in any languages, this functionality makes it multi-lingual since regardless of which language \acrshort{lilt} is pre-trained, it can be fine-tune using same methods in any language. Therefore to achieve the model that is performing best in German language

After the fine-tuning, we have the NLP model that is fine-tuned in German language and we can check whether the model is able to identify classes in different languages. 

% Due to the limitation of time, we have not included the evaluation of the model on various language dataset. However in \Cref{results_languages}, the results of \acrshort{lilt} on different languages is shown. We include example documents from English, Spanish, French, Italian and Portuguese. The results are not most accurate since the model does not know all languages but it is still able to identify some classes correctly by using layout knowledge learned from fine-tuning in German language dataset. Different classes are colored in different color boxes such as header is denoted by yellow, Question is denoted by blue,  Answer is denoted by green and other id denoted by violet. 