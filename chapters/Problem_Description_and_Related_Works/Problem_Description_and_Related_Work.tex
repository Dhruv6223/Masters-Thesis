\chapter{Background\_V1 \label{Chapter_Background}}

\large  The internet and computers have played significant role in a transformation of documents. Before documents got digitized, it used to be in form of printed papers. Most of the documents have their own layouts which is highly dependent on the language since the language comes with its own grammatical rules and the way of sentence formation. Once the smartphone came into the world, it started influencing the way of documents being prepared. To understand the text document has always been a part of interest in computer science. Since the impact of having a system that can understand the text document correctly, its repeatable functionality and speed that could help industries or companies to bring down the cost and time they are spending on dealing with documents on their day to day life. However, there are many steps in between to address before machine can read and understand text and layout information in text documents. Start with the letters, words and sentences. These are the basic and prime components of any language. One can not pass any information without it. One can argue about passing information in terms of symbol or a figure but at the basic level a letter is like a symbol or a figure that represents some value or information. We as a human being can easily recognise the texture and the layout of the letter by just looking at it, which is not the case for computers. Till the date computers are using 1s and 0s to perform tasks. Therefore it is a challenge to be able to recognise and process the letters that have been used in word or sentence while generating the document and form a structure of 1s and 0s so that machine can understand. 

Compilers are specifically design to understand high-level programming languages and translate it to the machine code, but when it comes to read a document, it is difficult for them since document does not contains the rules similar to programming languages and in addition, every text documents have the basic blocks of any human language, "letters". Character recognition techniques refers to a symbolic identity of a character within symbol or a character. This includes the replication/recognition of human functions like machine printed and hand printed/cursive-written characters by machines. Character recognition is known as optical character recognition since it deals with optically processed characters. The first mention of OCR was for as an aid to the visually handicapped and the first successful attempt was achieved by Russian Scientist Tyurin in 1900 \cite{govindan1990character}. Later on, the application of OCR changed into data processing systems in business world since it can deal with the enormous flood of paper documents such as bank cheques, commercial forms, credit card imprints, governmental records, mails and so on. Before smartphones and digital camera, documents were simply printed papers, However since smartphones and digital cameras got introduced, the number of documents increased since documents can now be easily produced digitally. Though generating documents got digitized, not all the documents are in machine readable or processable format. Most of them are just images or scans. Where OCR allowed to deal with these digitally born document such as scanned paper-documents, PDF files or images captured by a digital camera into machine readable, editable and searchable data. In paper \citep{AnOverviewoftheTesseractOCREngine} author have described in detail functionality of methods of famous Tesseract OCR Engine. Author talk about the development of open-source OCR engine "Tesseract" that it was developed at HP between 1984 and 1994. In the infancy era for Tesseract, the accuracy was a challenge, and it was major PhD research project in HP Labs Bristol. It was 1994, HP presented the final developed OCR engine to UNLV for the 1995 Annual Test of OCR Accuracy\cite{UNLV_4th_annual_test_ocr}, where it got proved that it was worth against the commercial engines that exists that time. Later in 2005, HP made Tesseract for open source which is available at \url{http://code.google.com/p/tesseract-ocr}. According to \cite{AnOverviewoftheTesseractOCREngine}, the functionality of Tesseract OCR will be described in CHAPTER METHODS.


OCR provides ability to extract the letters out of documents and compilers can translate the specific language structure in a way that machine can understand, However, there is some mechanism missing between compiling High-language programming language and understanding the letters, words or sentences derived from parsing the document. Language plays a dominant role in terms of passing information, thoughts and ideas. In addition, different regions, countries and part of the world have different languages that varies in structure, grammar, letters and so on making it difficult to come with the logic or mathematical rule which can apply on one or more languages in order to understand for machines. The early approach of Noam Chomsky \cite{robert1957review}, who introduced syntactic structures as a formalized theory of linguistic structure. He introduced rules based on universal grammar: However languages are generally not well-defined and lack in stable structure hence the approach had several drawbacks. Although, the approach was not perfect, it started the true evolution in NLP. Later on the trend shifted to using probabilities and statistics for machine translation but the progress was slower than expected resulting in less funding. It was the year 1969, Roger Schank presented the concept of using tokens \cite{tokenization_history} which is still being used in NLP, Tokens provided better grasp to map sentences, it can provide better insights to machine with detailed information at object-level. 

\input{chapters/Problem_Description_and_Related_Works/Document_understanding_for_V1}

\input{chapters/Problem_Description_and_Related_Works/deployment_background_cloud_computing}



